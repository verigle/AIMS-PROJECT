{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koomi_aims_ac_za/koomi/projects/AIMS-PROJECT/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "from aurora import AuroraSmall, Batch, Metadata, rollout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cdsapi\n",
    "import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import gcsfs\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from aurora import Batch, Metadata\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from utils import get_surface_feature_target_data, get_atmos_feature_target_data\n",
    "from utils import get_static_feature_target_data, create_batch, predict_fn, rmse_weights\n",
    "from utils import rmse_fn, plot_rmses, custom_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch import ERA5ZarrDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(token=\"anon\")\n",
    "\n",
    "store = fs.get_mapper('gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr')\n",
    "full_era5 = xr.open_zarr(store=store, consolidated=True, chunks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, end_time = '2022-12-01', '2023-01-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lat_max = -22.00 \n",
    "lat_min = -37.75  \n",
    "\n",
    "lon_min = 15.25   \n",
    "lon_max = 35.00   \n",
    "sliced_era5 = (\n",
    "    full_era5\n",
    "    .sel(\n",
    "        time=slice(start_time, end_time),\n",
    "        latitude=slice(lat_max, lat_min),\n",
    "        longitude=slice(lon_min, lon_max)  \n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ERA5ZarrDataset(sliced_era5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=8, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels = next(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aurora import AuroraSmall\n",
    "\n",
    "# model = AuroraSmall(\n",
    "#     use_lora=False,  # Model was not fine-tuned.\n",
    "#     autocast=True,  # Use AMP.\n",
    "# )\n",
    "# # model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-small-pretrained.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"../model/aurora-pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuroraSmall(\n",
    "    use_lora=False,  # Model was not fine-tuned.\n",
    "    autocast=True,  # Use AMP.\n",
    ")\n",
    "model.load_state_dict(torch.load('../model/aurora-pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    parameters, trainable = 0, 0\n",
    "    \n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "    print(trainable)\n",
    "    print(f\"trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112797584\n",
      "trainable parameters: 112,797,584/112,797,584 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "trainable parameters: 0/112,797,584 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.backbone.time_mlp.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131584\n",
      "trainable parameters: 131,584/112,797,584 (0.12%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.backbone.encoder_layers[0].blocks[0].norm1.ln_modulation.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263168\n",
      "trainable parameters: 263,168/112,797,584 (0.23%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get south africa Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(token=\"anon\")\n",
    "\n",
    "store = fs.get_mapper('gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr')\n",
    "full_era5 = xr.open_zarr(store=store, consolidated=True, chunks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, end_time = '2022-12-01', '2023-01-31'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "atmostpheric_variables = [\"temperature\", \"u_component_of_wind\", \"v_component_of_wind\", \"specific_humidity\", \"geopotential\"]\n",
    "surface_vars = ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'mean_sea_level_pressure']\n",
    "static_variables = [\"land_sea_mask\", \"soil_type\", \"geopotential_at_surface\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lat_max = -22.00 \n",
    "lat_min = -37.75  \n",
    "\n",
    "lon_min = 15.25   \n",
    "lon_max = 35.00   \n",
    "sliced_era5_SA = (\n",
    "    full_era5\n",
    "    .sel(\n",
    "        time=slice(start_time, end_time),\n",
    "        latitude=slice(lat_max, lat_min),\n",
    "        longitude=slice(lon_min, lon_max)  \n",
    "    )\n",
    "    .isel(time=slice(None, -2))\n",
    ")\n",
    "\n",
    "target_sliced_era5_SA = (\n",
    "    full_era5\n",
    "    .sel(\n",
    "        time=slice(start_time, end_time),\n",
    "        latitude=slice(lat_max, lat_min),\n",
    "        longitude=slice(lon_min, lon_max)  \n",
    "    )  \n",
    "    .isel(time=slice(2, None))  # Skip the first two time steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_vars_ds_SA = sliced_era5_SA[surface_vars]\n",
    "\n",
    "target_surf_vars_ds_SA = target_sliced_era5_SA[surface_vars]\n",
    "\n",
    "atmos_vars_ds_SA = sliced_era5_SA[atmostpheric_variables]\n",
    "\n",
    "target_atmos_vars_ds_SA = target_sliced_era5_SA[atmostpheric_variables]\n",
    "\n",
    "static_vars_ds_SA = sliced_era5_SA[static_variables]\n",
    "\n",
    "target_static_vars_ds_SA = target_sliced_era5_SA[static_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ERA5ZarrDataset(Dataset):\n",
    "    def __init__(self, surf_vars_ds, atmos_vars_ds, static_vars_ds, sequence_length):\n",
    "        self.surf_vars_ds = surf_vars_ds\n",
    "        self.atmos_vars_ds = atmos_vars_ds\n",
    "        self.static_vars_ds = static_vars_ds\n",
    "        self.sequence_length = sequence_length\n",
    "        self.time_indices = range(sequence_length, len(surf_vars_ds.time))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.time_indices[idx]\n",
    "\n",
    "        surf_vars = {\n",
    "            \"2t\": torch.from_numpy(self.surf_vars_ds[\"2m_temperature\"].values[[i - 1, i]][None]),\n",
    "            \"10u\": torch.from_numpy(self.surf_vars_ds[\"10m_u_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "            \"10v\": torch.from_numpy(self.surf_vars_ds[\"10m_v_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "            \"msl\": torch.from_numpy(self.surf_vars_ds[\"mean_sea_level_pressure\"].values[[i - 1, i]][None]),\n",
    "        }\n",
    "\n",
    "        static_vars = {\n",
    "            \"z\": torch.from_numpy(self.static_vars_ds[\"geopotential_at_surface\"].values),\n",
    "            \"slt\": torch.from_numpy(self.static_vars_ds[\"soil_type\"].values),\n",
    "            \"lsm\": torch.from_numpy(self.static_vars_ds[\"land_sea_mask\"].values),\n",
    "        }\n",
    "\n",
    "        atmos_vars = {\n",
    "            \"t\": torch.from_numpy(self.atmos_vars_ds[\"temperature\"].values[[i - 1, i]][None]),\n",
    "            \"u\": torch.from_numpy(self.atmos_vars_ds[\"u_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "            \"v\": torch.from_numpy(self.atmos_vars_ds[\"v_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "            \"q\": torch.from_numpy(self.atmos_vars_ds[\"specific_humidity\"].values[[i - 1, i]][None]),\n",
    "            \"z\": torch.from_numpy(self.atmos_vars_ds[\"geopotential\"].values[[i - 1, i]][None]),\n",
    "        }\n",
    "\n",
    "        metadata=Metadata(\n",
    "        lat=torch.from_numpy(self.surf_vars_ds.latitude.values),\n",
    "        lon=torch.from_numpy(self.surf_vars_ds.longitude.values),\n",
    "        time=(self.surf_vars_ds.time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in self.atmos_vars_ds.level.values)\n",
    "    )\n",
    "\n",
    "\n",
    "        return Batch(surf_vars=surf_vars, static_vars=static_vars, atmos_vars=atmos_vars, metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_batches = ERA5ZarrDataset(surf_vars_ds_SA, atmos_vars_ds_SA, static_vars_ds_SA,1)\n",
    "target_SA_batches = ERA5ZarrDataset(target_surf_vars_ds_SA, target_atmos_vars_ds_SA, target_static_vars_ds_SA,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, original_layer, rank=4):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, original_layer.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.randn(original_layer.out_features, rank))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.original_layer(x) + (x @ self.lora_A.T) @ self.lora_B.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_model(model, rank=4):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, LoRA(module, rank))\n",
    "        else:\n",
    "            apply_lora_to_model(module, rank)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = apply_lora_to_model(model, rank=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_lora_to_model(model, rank=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285184\n",
      "trainable parameters: 2,285,184/114,951,184 (1.99%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(1):\n",
    "#     print(\"start\")\n",
    "#     for inputs, targets in zip(SA_batches, target_SA_batches):\n",
    "#         print(\"OK\")\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         print(loss)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"aurora_lora_finetuned.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sa_latitudes, sa_longitudes = sliced_era5_SA.latitude, sliced_era5_SA.longitude\n",
    "\n",
    "# # Compute RMSE weights\n",
    "# sa_rmse_weights = rmse_weights(sa_latitudes, sa_longitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import AuroraLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = AuroraLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Prediction done\n",
      "tensor(9.9808e+08, grad_fn=<MulBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(1.0561e+09, grad_fn=<MulBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(9.0960e+08, grad_fn=<MulBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(9.5782e+08, grad_fn=<MulBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(1.0720e+09, grad_fn=<MulBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(9.3800e+08, grad_fn=<MulBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(1.0446e+09, grad_fn=<MulBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(1.0078e+09, grad_fn=<MulBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(1.0679e+09, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sa_latitudes = target_sliced_era5_SA.latitude\n",
    "sa_longitudes = target_sliced_era5_SA.longitude\n",
    "\n",
    "selected_times =  target_sliced_era5_SA.time\n",
    "sa_rmses_list=[]\n",
    "for i in range(0, len(target_sliced_era5_SA.time)-3):\n",
    "    # get current and previous time step data\n",
    "    world_feature_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "    sa_feature_data =  (\n",
    "            sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "\n",
    "    sa_target_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i+2], selected_times[i+3]))\n",
    "        )\n",
    "    \n",
    "    # get each type of data(surface, static atmosphere)\n",
    "\n",
    "    sa_feature_surface_data, sa_target_surface_data = get_surface_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_atmos_data, sa_target_atmos_data = get_atmos_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_static_data, sa_target_static_data = get_static_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    \n",
    "    # create batch for each of them\n",
    "\n",
    "    sa_feature_bacth =  create_batch(sa_feature_surface_data, sa_feature_atmos_data, sa_feature_static_data)\n",
    "    sa_target_bacth = create_batch(sa_target_surface_data, sa_target_atmos_data, sa_target_static_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Start\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(sa_feature_bacth)\n",
    "    print(\"Prediction done\")\n",
    "    loss = criterion(outputs, sa_target_bacth, \"ERA5\")\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Prediction done\n",
      "tensor(80327112., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(73870288., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(71279416., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(66979388., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(76102832., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(67231240., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(58614704., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(55923936., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(53904288., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(52293720., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(53105628., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(49706236., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(47634212., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(41055844., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(38299132., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(33816268., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(39691012., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(42012164., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(33388950., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(34213344., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(30999476., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(31589654., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(29364390., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(28333002., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(25883004., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(29208592., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(25703238., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(27869326., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(26632116., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(25687588., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(32170082., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(29419610., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(40245000., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(26212446., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(32116296., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(31494134., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(30765200., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(26905836., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(23809776., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(24449230., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22905530., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21791972., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22354418., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(25172724., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(24263340., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21916140., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22620568., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(27734970., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21010828., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22029784., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22428246., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21524434., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22586060., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21530960., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(26353092., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21127948., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21982366., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21236542., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22196124., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20656078., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20417586., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18970948., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20080552., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20694100., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20440400., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22566104., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(17326216., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(19529356., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18972038., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21992630., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18460864., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(19171774., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(19396044., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18437434., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20611096., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(19352006., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22980710., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(17860186., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(17809980., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18167484., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18144598., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18782698., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(17782434., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21183594., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22130520., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(17689530., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18731296., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(19827146., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(16978054., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22844256., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(19689590., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20758764., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(17735108., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(18248282., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21786624., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20930404., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21524638., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20834290., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(19458034., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21708878., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20302614., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(20814962., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(21554094., grad_fn=<SqrtBackward0>)\n",
      "Start\n",
      "Prediction done\n",
      "tensor(22205518., grad_fn=<SqrtBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_rmse(target_tensor, output_tensor, sa_rmse_weights)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/koomi/projects/AIMS-PROJECT/venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/koomi/projects/AIMS-PROJECT/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/koomi/projects/AIMS-PROJECT/venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "sa_latitudes = target_sliced_era5_SA.latitude\n",
    "sa_longitudes = target_sliced_era5_SA.longitude\n",
    "\n",
    "selected_times =  target_sliced_era5_SA.time\n",
    "sa_rmses_list=[]\n",
    "for i in range(0, len(target_sliced_era5_SA.time)-3):\n",
    "    # get current and previous time step data\n",
    "    world_feature_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "    sa_feature_data =  (\n",
    "            sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "\n",
    "    sa_target_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i+2], selected_times[i+3]))\n",
    "        )\n",
    "    \n",
    "    # get each type of data(surface, static atmosphere)\n",
    "\n",
    "    sa_feature_surface_data, sa_target_surface_data = get_surface_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_atmos_data, sa_target_atmos_data = get_atmos_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_static_data, sa_target_static_data = get_static_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    \n",
    "    # create batch for each of them\n",
    "\n",
    "    sa_feature_bacth =  create_batch(sa_feature_surface_data, sa_feature_atmos_data, sa_feature_static_data)\n",
    "    sa_target_bacth = create_batch(sa_target_surface_data, sa_target_atmos_data, sa_target_static_data)\n",
    "    \n",
    "    \n",
    "    target_tensor = sa_target_bacth.surf_vars[\"2t\"].squeeze()[1,:,:]\n",
    "    \n",
    "    print(\"Start\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(sa_feature_bacth)\n",
    "    output_tensor =  outputs.surf_vars[\"2t\"][0, 0]\n",
    "    print(\"Prediction done\")\n",
    "    loss = custom_rmse(target_tensor, output_tensor, sa_rmse_weights)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(sa_feature_bacth)\n\u001b[1;32m     51\u001b[0m loss , _ \u001b[38;5;241m=\u001b[39m rmse_fn(predictions\u001b[38;5;241m=\u001b[39m[pred], \n\u001b[1;32m     52\u001b[0m          target_batch\u001b[38;5;241m=\u001b[39msa_target_bacth, var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2t\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m          weigths\u001b[38;5;241m=\u001b[39msa_rmse_weights, area\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "\n",
    "sa_latitudes = target_sliced_era5_SA.latitude\n",
    "sa_longitudes = target_sliced_era5_SA.longitude\n",
    "sa_rmse_weights = rmse_weights(sa_latitudes, sa_longitudes)\n",
    "selected_times =  target_sliced_era5_SA.time\n",
    "sa_rmses_list=[]\n",
    "for i in range(0, len(target_sliced_era5_SA.time)-3):\n",
    "    # get current and previous time step data\n",
    "    world_feature_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "    sa_feature_data =  (\n",
    "            sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "\n",
    "    sa_target_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i+2], selected_times[i+3]))\n",
    "        )\n",
    "    \n",
    "    # get each type of data(surface, static atmosphere)\n",
    "\n",
    "    sa_feature_surface_data, sa_target_surface_data = get_surface_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_atmos_data, sa_target_atmos_data = get_atmos_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_static_data, sa_target_static_data = get_static_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    \n",
    "    # create batch for each of them\n",
    "\n",
    "    sa_feature_bacth =  create_batch(sa_feature_surface_data, sa_feature_atmos_data, sa_feature_static_data)\n",
    "    sa_target_bacth = create_batch(sa_target_surface_data, sa_target_atmos_data, sa_target_static_data)\n",
    "    # get prediction\n",
    "    # sa_predictions = predict_fn(batch=sa_feature_bacth)\n",
    "    # # compute the rmse\n",
    "    \n",
    "    # sa_rmses, sa_pred_dates = rmse_fn(predictions=sa_predictions, \n",
    "    #         target_batch=sa_target_bacth, var_name=\"2t\",\n",
    "    #         weigths=sa_rmse_weights, area=\"sa\")\n",
    "    # # append result to the list\n",
    "    # world_rmses_list.append(world_rmses); pred_dates_list.append(world_pred_dates)\n",
    "    # sa_rmses_list.append(sa_rmses)\n",
    "    print(\"Start\")\n",
    "    \n",
    "\n",
    "\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    model.configure_activation_checkpointing()\n",
    "\n",
    "    pred = model.forward(sa_feature_bacth)\n",
    "    loss , _ = rmse_fn(predictions=[pred], \n",
    "             target_batch=sa_target_bacth, var_name=\"2t\",\n",
    "             weigths=sa_rmse_weights, area=\"sa\")\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_rmses, _ = rmse_fn(predictions=pred, \n",
    "             target_batch=sa_target_bacth, var_name=\"2t\",\n",
    "             weigths=sa_rmse_weights, area=\"sa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
