{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koomi_aims_ac_za/koomi/projects/AIMS-PROJECT/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "from aurora import AuroraSmall, Batch, Metadata, rollout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cdsapi\n",
    "import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import gcsfs\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from aurora import Batch, Metadata\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from utils import get_surface_feature_target_data, get_atmos_feature_target_data\n",
    "from utils import get_static_feature_target_data, create_batch, predict_fn, rmse_weights\n",
    "from utils import rmse_fn, plot_rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import AuroraSmall\n",
    "\n",
    "model = AuroraSmall(\n",
    "    use_lora=False,  # Model was not fine-tuned.\n",
    "    autocast=True,  # Use AMP.\n",
    ")\n",
    "# model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-small-pretrained.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"../model/aurora-pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuroraSmall(\n",
    "    use_lora=False,  # Model was not fine-tuned.\n",
    "    autocast=True,  # Use AMP.\n",
    ")\n",
    "model.load_state_dict(torch.load('../model/aurora-pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    parameters, trainable = 0, 0\n",
    "    \n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "    print(trainable)\n",
    "    print(f\"trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112797584\n",
      "trainable parameters: 112,797,584/112,797,584 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "trainable parameters: 0/112,797,584 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.backbone.time_mlp.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114951184\n",
      "trainable parameters: 114,951,184/114,951,184 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.backbone.encoder_layers[0].blocks[0].norm1.ln_modulation.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263168\n",
      "trainable parameters: 263,168/112,797,584 (0.23%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get south africa Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(token=\"anon\")\n",
    "\n",
    "store = fs.get_mapper('gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr')\n",
    "full_era5 = xr.open_zarr(store=store, consolidated=True, chunks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, end_time = '2022-12-01', '2023-01-31'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "atmostpheric_variables = [\"temperature\", \"u_component_of_wind\", \"v_component_of_wind\", \"specific_humidity\", \"geopotential\"]\n",
    "surface_vars = ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'mean_sea_level_pressure']\n",
    "static_variables = [\"land_sea_mask\", \"soil_type\", \"geopotential_at_surface\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lat_max = -22.00 \n",
    "lat_min = -37.75  \n",
    "\n",
    "lon_min = 15.25   \n",
    "lon_max = 35.00   \n",
    "sliced_era5_SA = (\n",
    "    full_era5\n",
    "    .sel(\n",
    "        time=slice(start_time, end_time),\n",
    "        latitude=slice(lat_max, lat_min),\n",
    "        longitude=slice(lon_min, lon_max)  \n",
    "    )\n",
    "    .isel(time=slice(None, -2))\n",
    ")\n",
    "\n",
    "target_sliced_era5_SA = (\n",
    "    full_era5\n",
    "    .sel(\n",
    "        time=slice(start_time, end_time),\n",
    "        latitude=slice(lat_max, lat_min),\n",
    "        longitude=slice(lon_min, lon_max)  \n",
    "    )  \n",
    "    .isel(time=slice(2, None))  # Skip the first two time steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_vars_ds_SA = sliced_era5_SA[surface_vars]\n",
    "\n",
    "target_surf_vars_ds_SA = target_sliced_era5_SA[surface_vars]\n",
    "\n",
    "atmos_vars_ds_SA = sliced_era5_SA[atmostpheric_variables]\n",
    "\n",
    "target_atmos_vars_ds_SA = target_sliced_era5_SA[atmostpheric_variables]\n",
    "\n",
    "static_vars_ds_SA = sliced_era5_SA[static_variables]\n",
    "\n",
    "target_static_vars_ds_SA = target_sliced_era5_SA[static_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ERA5ZarrDataset(Dataset):\n",
    "    def __init__(self, surf_vars_ds, atmos_vars_ds, static_vars_ds, sequence_length):\n",
    "        self.surf_vars_ds = surf_vars_ds\n",
    "        self.atmos_vars_ds = atmos_vars_ds\n",
    "        self.static_vars_ds = static_vars_ds\n",
    "        self.sequence_length = sequence_length\n",
    "        self.time_indices = range(sequence_length, len(surf_vars_ds.time))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.time_indices[idx]\n",
    "\n",
    "        surf_vars = {\n",
    "            \"2t\": torch.from_numpy(self.surf_vars_ds[\"2m_temperature\"].values[[i - 1, i]][None]),\n",
    "            \"10u\": torch.from_numpy(self.surf_vars_ds[\"10m_u_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "            \"10v\": torch.from_numpy(self.surf_vars_ds[\"10m_v_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "            \"msl\": torch.from_numpy(self.surf_vars_ds[\"mean_sea_level_pressure\"].values[[i - 1, i]][None]),\n",
    "        }\n",
    "\n",
    "        static_vars = {\n",
    "            \"z\": torch.from_numpy(self.static_vars_ds[\"geopotential_at_surface\"].values),\n",
    "            \"slt\": torch.from_numpy(self.static_vars_ds[\"soil_type\"].values),\n",
    "            \"lsm\": torch.from_numpy(self.static_vars_ds[\"land_sea_mask\"].values),\n",
    "        }\n",
    "\n",
    "        atmos_vars = {\n",
    "            \"t\": torch.from_numpy(self.atmos_vars_ds[\"temperature\"].values[[i - 1, i]][None]),\n",
    "            \"u\": torch.from_numpy(self.atmos_vars_ds[\"u_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "            \"v\": torch.from_numpy(self.atmos_vars_ds[\"v_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "            \"q\": torch.from_numpy(self.atmos_vars_ds[\"specific_humidity\"].values[[i - 1, i]][None]),\n",
    "            \"z\": torch.from_numpy(self.atmos_vars_ds[\"geopotential\"].values[[i - 1, i]][None]),\n",
    "        }\n",
    "\n",
    "        metadata=Metadata(\n",
    "        lat=torch.from_numpy(self.surf_vars_ds.latitude.values),\n",
    "        lon=torch.from_numpy(self.surf_vars_ds.longitude.values),\n",
    "        time=(self.surf_vars_ds.time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in self.atmos_vars_ds.level.values)\n",
    "    )\n",
    "\n",
    "\n",
    "        return Batch(surf_vars=surf_vars, static_vars=static_vars, atmos_vars=atmos_vars, metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_batches = ERA5ZarrDataset(surf_vars_ds_SA, atmos_vars_ds_SA, static_vars_ds_SA,1)\n",
    "target_SA_batches = ERA5ZarrDataset(target_surf_vars_ds_SA, target_atmos_vars_ds_SA, target_static_vars_ds_SA,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, original_layer, rank=4):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, original_layer.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.randn(original_layer.out_features, rank))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.original_layer(x) + (x @ self.lora_A.T) @ self.lora_B.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_model(model, rank=4):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, LoRA(module, rank))\n",
    "        else:\n",
    "            apply_lora_to_model(module, rank)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = apply_lora_to_model(model, rank=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_lora_to_model(model, rank=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2153600\n",
      "trainable parameters: 2,153,600/114,951,184 (1.87%)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    print(\"start\")\n",
    "    for inputs, targets in zip(SA_batches, target_SA_batches):\n",
    "        print(\"OK\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"aurora_lora_finetuned.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sa_latitudes = target_sliced_era5_SA.latitude\n",
    "sa_longitudes = target_sliced_era5_SA.longitude\n",
    "\n",
    "selected_times =  target_sliced_era5_SA.time\n",
    "sa_rmses_list=[]\n",
    "for i in range(0, len(target_sliced_era5_SA.time)-3):\n",
    "    # get current and previous time step data\n",
    "    world_feature_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "    sa_feature_data =  (\n",
    "            sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "\n",
    "    sa_target_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i+2], selected_times[i+3]))\n",
    "        )\n",
    "    \n",
    "    # get each type of data(surface, static atmosphere)\n",
    "\n",
    "    sa_feature_surface_data, sa_target_surface_data = get_surface_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_atmos_data, sa_target_atmos_data = get_atmos_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_static_data, sa_target_static_data = get_static_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    \n",
    "    # create batch for each of them\n",
    "\n",
    "    sa_feature_bacth =  create_batch(sa_feature_surface_data, sa_feature_atmos_data, sa_feature_static_data)\n",
    "    sa_target_bacth = create_batch(sa_target_surface_data, sa_target_atmos_data, sa_target_static_data)\n",
    "    # get prediction\n",
    "    # sa_predictions = predict_fn(batch=sa_feature_bacth)\n",
    "    # # compute the rmse\n",
    "    \n",
    "    # sa_rmses, sa_pred_dates = rmse_fn(predictions=sa_predictions, \n",
    "    #         target_batch=sa_target_bacth, var_name=\"2t\",\n",
    "    #         weigths=sa_rmse_weights, area=\"sa\")\n",
    "    # # append result to the list\n",
    "    # world_rmses_list.append(world_rmses); pred_dates_list.append(world_pred_dates)\n",
    "    # sa_rmses_list.append(sa_rmses)\n",
    "    print(\"Start\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(sa_feature_bacth)\n",
    "    loss = criterion(outputs, sa_target_bacth)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# model.configure_activation_checkpointing()\n",
    "\n",
    "# pred = model.forward(batch)\n",
    "# loss = ...\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(sa_feature_bacth)\n\u001b[1;32m     51\u001b[0m loss , _ \u001b[38;5;241m=\u001b[39m rmse_fn(predictions\u001b[38;5;241m=\u001b[39m[pred], \n\u001b[1;32m     52\u001b[0m          target_batch\u001b[38;5;241m=\u001b[39msa_target_bacth, var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2t\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m          weigths\u001b[38;5;241m=\u001b[39msa_rmse_weights, area\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "\n",
    "sa_latitudes = target_sliced_era5_SA.latitude\n",
    "sa_longitudes = target_sliced_era5_SA.longitude\n",
    "sa_rmse_weights = rmse_weights(sa_latitudes, sa_longitudes)\n",
    "selected_times =  target_sliced_era5_SA.time\n",
    "sa_rmses_list=[]\n",
    "for i in range(0, len(target_sliced_era5_SA.time)-3):\n",
    "    # get current and previous time step data\n",
    "    world_feature_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "    sa_feature_data =  (\n",
    "            sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i], selected_times[i+1]))\n",
    "        )\n",
    "\n",
    "    sa_target_data =  (\n",
    "            target_sliced_era5_SA\n",
    "            .sel(time=slice(selected_times[i+2], selected_times[i+3]))\n",
    "        )\n",
    "    \n",
    "    # get each type of data(surface, static atmosphere)\n",
    "\n",
    "    sa_feature_surface_data, sa_target_surface_data = get_surface_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_atmos_data, sa_target_atmos_data = get_atmos_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    sa_feature_static_data, sa_target_static_data = get_static_feature_target_data(sa_feature_data, sa_target_data)\n",
    "    \n",
    "    # create batch for each of them\n",
    "\n",
    "    sa_feature_bacth =  create_batch(sa_feature_surface_data, sa_feature_atmos_data, sa_feature_static_data)\n",
    "    sa_target_bacth = create_batch(sa_target_surface_data, sa_target_atmos_data, sa_target_static_data)\n",
    "    # get prediction\n",
    "    # sa_predictions = predict_fn(batch=sa_feature_bacth)\n",
    "    # # compute the rmse\n",
    "    \n",
    "    # sa_rmses, sa_pred_dates = rmse_fn(predictions=sa_predictions, \n",
    "    #         target_batch=sa_target_bacth, var_name=\"2t\",\n",
    "    #         weigths=sa_rmse_weights, area=\"sa\")\n",
    "    # # append result to the list\n",
    "    # world_rmses_list.append(world_rmses); pred_dates_list.append(world_pred_dates)\n",
    "    # sa_rmses_list.append(sa_rmses)\n",
    "    print(\"Start\")\n",
    "    \n",
    "\n",
    "\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    model.configure_activation_checkpointing()\n",
    "\n",
    "    pred = model.forward(sa_feature_bacth)\n",
    "    loss , _ = rmse_fn(predictions=[pred], \n",
    "             target_batch=sa_target_bacth, var_name=\"2t\",\n",
    "             weigths=sa_rmse_weights, area=\"sa\")\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_rmses, _ = rmse_fn(predictions=pred, \n",
    "             target_batch=sa_target_bacth, var_name=\"2t\",\n",
    "             weigths=sa_rmse_weights, area=\"sa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
